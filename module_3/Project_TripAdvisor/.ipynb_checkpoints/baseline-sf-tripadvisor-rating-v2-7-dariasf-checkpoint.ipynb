{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.pata.org/wp-content/uploads/2014/09/TripAdvisor_Logo-300x119.png)\n",
    "# Predict TripAdvisor Rating\n",
    "## В этом соревновании нам предстоит предсказать рейтинг ресторана в TripAdvisor\n",
    "**По ходу задачи:**\n",
    "* Прокачаем работу с pandas\n",
    "* Научимся работать с Kaggle Notebooks\n",
    "* Поймем как делать предобработку различных данных\n",
    "* Научимся работать с пропущенными данными (Nan)\n",
    "* Познакомимся с различными видами кодирования признаков\n",
    "* Немного попробуем [Feature Engineering](https://ru.wikipedia.org/wiki/Конструирование_признаков) (генерировать новые признаки)\n",
    "* И совсем немного затронем ML\n",
    "* И многое другое...   \n",
    "\n",
    "\n",
    "\n",
    "## Проект подготовлен Дарьей Фесенко "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорт необходимых библиотек и модулей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-09-28T19:06:43.783349Z",
     "iopub.status.busy": "2021-09-28T19:06:43.782775Z",
     "iopub.status.idle": "2021-09-28T19:07:34.87898Z",
     "shell.execute_reply": "2021-09-28T19:07:34.878122Z",
     "shell.execute_reply.started": "2021-09-28T19:06:43.783296Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np # linear algebra\n",
    "import re\n",
    "from re import findall\n",
    "import datetime\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import os\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora\n",
    "sw = stopwords.words('english')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "import pandas_profiling\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "import cufflinks as cf\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly.express as px\n",
    "import seaborn as sns \n",
    "%matplotlib inline\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "cf.go_offline()\n",
    "\n",
    "# Загружаем специальный удобный инструмент для разделения датасета:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import math\n",
    "import copy\n",
    "from IPython.display import display\n",
    "pd.options.mode.chained_assignment = None\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_columns', 200)  # показывать больше колонок\n",
    "RANDOM_SEED = 581321\n",
    "!pip freeze > requirements.txt\n",
    "# Any results you write to the current directory are saved as output.\n",
    "CURRENT_DATE = pd.to_datetime('01/10/2021')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-09-28T19:07:34.881193Z",
     "iopub.status.busy": "2021-09-28T19:07:34.880926Z",
     "iopub.status.idle": "2021-09-28T19:07:35.296015Z",
     "shell.execute_reply": "2021-09-28T19:07:35.295142Z",
     "shell.execute_reply.started": "2021-09-28T19:07:34.881153Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('main_task.csv')\n",
    "df_test = pd.read_csv('kaggle_task.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:07:35.353252Z",
     "iopub.status.busy": "2021-09-28T19:07:35.352861Z",
     "iopub.status.idle": "2021-09-28T19:07:35.384015Z",
     "shell.execute_reply": "2021-09-28T19:07:35.382977Z",
     "shell.execute_reply.started": "2021-09-28T19:07:35.353088Z"
    }
   },
   "outputs": [],
   "source": [
    "display(df_train.head(2))\n",
    "display(df_test.head(2))\n",
    "display(sample_submission.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:07:35.387729Z",
     "iopub.status.busy": "2021-09-28T19:07:35.387434Z",
     "iopub.status.idle": "2021-09-28T19:07:35.405407Z",
     "shell.execute_reply": "2021-09-28T19:07:35.404135Z",
     "shell.execute_reply.started": "2021-09-28T19:07:35.387683Z"
    }
   },
   "outputs": [],
   "source": [
    "display(df_train.info())\n",
    "display(df_test.info())\n",
    "display(sample_submission.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:07:35.466689Z",
     "iopub.status.busy": "2021-09-28T19:07:35.466372Z",
     "iopub.status.idle": "2021-09-28T19:07:35.522352Z",
     "shell.execute_reply": "2021-09-28T19:07:35.52131Z",
     "shell.execute_reply.started": "2021-09-28T19:07:35.466638Z"
    }
   },
   "outputs": [],
   "source": [
    "# ВАЖНО! дря корректной обработки признаков объединяем трейн и тест в один датасет\n",
    "df_train['sample'] = 1 # помечаем где у нас трейн\n",
    "df_test['sample'] = 0 # помечаем где у нас тест\n",
    "df_test['Rating'] = 0 # в тесте у нас нет значения Rating, мы его должны предсказать, по этому пока просто заполняем нулями\n",
    "\n",
    "data = df_test.append(df_train, sort=False).reset_index(drop=True) # объединяем\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:07:35.524341Z",
     "iopub.status.busy": "2021-09-28T19:07:35.524001Z",
     "iopub.status.idle": "2021-09-28T19:07:35.569233Z",
     "shell.execute_reply": "2021-09-28T19:07:35.568428Z",
     "shell.execute_reply.started": "2021-09-28T19:07:35.524237Z"
    }
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подробнее по признакам:\n",
    "* City: Город \n",
    "* Cuisine Style: Кухня\n",
    "* Ranking: Ранг ресторана относительно других ресторанов в этом городе\n",
    "* Price Range: Цены в ресторане в 3 категориях\n",
    "* Number of Reviews: Количество отзывов\n",
    "* Reviews: 2 последних отзыва и даты этих отзывов\n",
    "* URL_TA: страница ресторана на 'www.tripadvisor.com' \n",
    "* ID_TA: ID ресторана в TripAdvisor\n",
    "* Rating: Рейтинг ресторана"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:07:35.591202Z",
     "iopub.status.busy": "2021-09-28T19:07:35.590862Z",
     "iopub.status.idle": "2021-09-28T19:07:35.619495Z",
     "shell.execute_reply": "2021-09-28T19:07:35.618309Z",
     "shell.execute_reply.started": "2021-09-28T19:07:35.591146Z"
    }
   },
   "outputs": [],
   "source": [
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:07:35.621189Z",
     "iopub.status.busy": "2021-09-28T19:07:35.620823Z",
     "iopub.status.idle": "2021-09-28T19:07:35.629002Z",
     "shell.execute_reply": "2021-09-28T19:07:35.627877Z",
     "shell.execute_reply.started": "2021-09-28T19:07:35.621118Z"
    }
   },
   "outputs": [],
   "source": [
    "data.Reviews[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, большинство признаков у нас требует очистки и предварительной обработки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Prepping Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0   Restaurant_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= data.copy()\n",
    "df.columns = [column.replace(' ','_') for column in df.columns]\n",
    "df.Restaurant_id.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, многие повторяются, видимо, есть сетевые магазины"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_franchise = df['Restaurant_id'].value_counts()\n",
    "df['count_franchise'] = df['Restaurant_id'].apply(lambda x: count_franchise[x])\n",
    "df['count_franchise'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1   City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(df['City'])\n",
    "df['code_City'] = le.transform(df['City'])\n",
    "\n",
    "count_rest_in_city = df['City'].value_counts()\n",
    "df['count_rest_in_city'] = df['City'].apply(lambda x: count_rest_in_city[x])\n",
    "df['count_rest_in_city'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,7))\n",
    "sns.boxplot(x='City', y='Rating', data=df[df['sample']==1])\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим признак с населением, тыс. человек. Данные из Википедии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_dict = {'Paris': 2148.327, 'Helsinki':656.611, 'Edinburgh':488.1, 'London':8961.989, 'Bratislava':437.725, \n",
    "             'Lisbon':505.526,'Budapest':1752.286, 'Stockholm':961.609, 'Rome':2870.500, 'Milan':1378.689, \n",
    "             'Munich':1471.508,'Hamburg':1841.179,'Prague':1335.084, 'Vienna':1897.491, 'Dublin':1173.179, \n",
    "             'Barcelona':1664.182, 'Brussels':185.103,'Madrid':3266.126,'Oslo':673.469, 'Amsterdam':872.757, \n",
    "             'Berlin':3644.826, 'Lyon': 506.615, 'Athens':664.046, 'Warsaw':1789.620,\n",
    "             'Oporto':231.962, 'Krakow':769.307, 'Copenhagen':1358.608, 'Luxembourg':636.739, \n",
    "             'Zurich':1407.572, 'Geneva':620.131,'Ljubljana':284.355}\n",
    "\n",
    "df['Population'] = df.apply(lambda row: city_dict[row['City']], axis=1)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dum_Cit = pd.get_dummies(df['City'], dummy_na=False).astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2   Cuisine_Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cuisine_Style'] = df['Cuisine_Style'].fillna(\"['cuis_isNaN']\")\n",
    "le.fit(df.Cuisine_Style)\n",
    "df['code_Cuisine'] = le.transform(df['Cuisine_Style'])\n",
    "df['Cuisine_Style'] = df['Cuisine_Style'].str.findall(r\"'(\\b.*?\\b)'\")\n",
    "df['Count_cuisine'] = df.Cuisine_Style.apply(lambda x: len(x))\n",
    "\n",
    "# функция для списка со списками\n",
    "def list_unrar(list_of_lists):\n",
    "    result=[]\n",
    "    for lst in list_of_lists:\n",
    "        result.extend(lst)\n",
    "    return result\n",
    "\n",
    "temp_list = df.Cuisine_Style.tolist()\n",
    "temp_Counter = Counter(list_unrar(temp_list))\n",
    "\n",
    "plt.figure(figsize=(18,7))\n",
    "df.explode('Cuisine_Style')['Cuisine_Style'].value_counts(ascending=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# список самых популярных кухонь\n",
    "list_of_popular = [x[0] for x in temp_Counter.most_common(15)]\n",
    "print(list_of_popular, len(list_of_popular))\n",
    "# список довольно редких кухонь (менее 90 упоминаний)\n",
    "list_of_niche = [x[0] for x in temp_Counter.most_common()[-50:]]\n",
    "print(list_of_niche, len(list_of_niche))\n",
    "# список кухонь средней частоты (все остальные)\n",
    "list_of_regular = [x[0] for x in temp_Counter.most_common()[15:-50]]\n",
    "print(list_of_regular, len(list_of_regular))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сформируем новые признаки - редковстречающиеся и среднечасто встречающиеся кухни объединим \n",
    "# в соответсвующие категории\n",
    "df['niche_Cuisine_Style'] = df['Cuisine_Style'].apply(lambda x: 1 if len(set(x) & set(list_of_niche))>0  else 0).astype('float64')\n",
    "\n",
    "df['regular_Cuisine_Style'] = df['Cuisine_Style'].apply(lambda x: 1 if len(set(x) & set(list_of_regular))>0  else 0).astype('float64')\n",
    "\n",
    "# а самые популярные 15 кухонь выведем в отдельные категории\n",
    "\n",
    "for cuisine in list_of_popular:\n",
    "    df[cuisine] = df['Cuisine_Style'].apply(lambda x: 1 if cuisine in x else 0 ).astype('float64')\n",
    "    \n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3   Ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отнормируем по городам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_Ranking_on_City = df.groupby(['City'])['Ranking'].mean()\n",
    "df['mean_Ranking_on_City'] = df['City'].apply(lambda x: mean_Ranking_on_City[x])\n",
    "df['norm_Ranking_on_Rest_in_City'] = (df['Ranking'] - df['mean_Ranking_on_City']) / df['count_rest_in_city']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отнормируем по населению в городе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['norm_Ranking_on_Pop_in_City'] = (df['Ranking'] - df['mean_Ranking_on_City']) / df['Population']\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4   Price Range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполним пропуски и выделим ценовые категории\n",
    "def price_range(x):\n",
    "    if x=='$':\n",
    "        return 'econom'\n",
    "    elif x=='$$ - $$$':\n",
    "        return 'regular'\n",
    "    elif x=='$$$$':\n",
    "        return 'luxury'\n",
    "    return \"price_isNaN\"\n",
    "\n",
    "df.Price_Range = df.Price_Range.apply(price_range)\n",
    "print(df.Price_Range.value_counts())\n",
    "sns.boxplot(x='Price_Range', y='Rating', data=df[df['sample']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_list = list(set(df.Price_Range.tolist()))\n",
    "for price in price_list:\n",
    "    df[price] = df['Price_Range'].apply(lambda x: 1 if price in x else 0 ).astype('float64') \n",
    "    \n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  5   Number_of_Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраним пропуски\n",
    "df['Number_of_Reviews_isNAN'] = pd.isna(df['Number_of_Reviews']).astype('float64')\n",
    "# заполним пропуски медианным по городу значением\n",
    "grp = df.groupby(['City'])\n",
    "df.Number_of_Reviews = grp.Number_of_Reviews.apply(lambda x: x.fillna(x.median()))\n",
    "df.groupby('City')['Number_of_Reviews'].sum().sort_values(ascending=False).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6   Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим даты отзывов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Reviews'] = df['Reviews'].fillna(\"[[], []]\")\n",
    "# найдем все даты в отзывах\n",
    "df['Dates'] = df.Reviews.str.findall(r'\\d{2}/\\d{2}/\\d{4}')\n",
    "df['Dates_num'] = df['Dates'].apply(len)\n",
    "df['Dates_num'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраним рестораны с одним отзывом\n",
    "df['one_Review'] = (df['Dates_num']==1).astype('float64')\n",
    "# функция определяет насколько давно был сделан самый свежий отзыв\n",
    "def time_to_now(row):\n",
    "    if row['Dates'] == []:\n",
    "        return None\n",
    "    return datetime.datetime.now() - pd.to_datetime(row['Dates']).max()\n",
    "# функция для определения перерыва между отзывами\n",
    "def time_between_Reviews(row):\n",
    "    if row['Dates'] == []:\n",
    "        return None\n",
    "    return pd.to_datetime(row['Dates']).max() - pd.to_datetime(row['Dates']).min()\n",
    "\n",
    "df['time_to_now'] = df.apply(time_to_now, axis = 1).dt.days\n",
    "print(df['time_to_now'].describe())\n",
    "df['Period_rev'] = df[df['Dates_num']==2].apply(time_between_Reviews, axis = 1).dt.days\n",
    "print(df['Period_rev'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# заполним пропуски средним значением по городу\n",
    "grp = df.groupby(['City'])\n",
    "df.time_to_now = grp.time_to_now.apply(lambda x: x.fillna(x.median()))\n",
    "df.Period_rev = grp.Period_rev.apply(lambda x: x.fillna(x.median()))\n",
    "df['time_to_now'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Period_rev'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теперь поработаем над текстом отзывов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция выдаст список слов отзыва сокращенных до корневой формы\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))  \n",
    "# функция удалит шумовые слова из текста\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "df['no_review'] = (df['Dates_num']==0).astype('float64')\n",
    "df['Text_clean'] = df['Reviews'].apply(lemmatize_stemming)\n",
    "df['Text_clean'] = df['Reviews'].apply(preprocess)\n",
    "df['Text_clean_len'] = df['Text_clean'].apply(len) \n",
    "print(df['Text_clean_len'].value_counts())\n",
    "df['Text_clean_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = df['Text_clean'].tolist()\n",
    "text_cnt = Counter(list_unrar(text_list))\n",
    "print(len(text_cnt.most_common()))\n",
    "text_cnt.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выделим списки популярных слов в соответствии с оценкой\n",
    "positive = ['good','great','nice','best','excel','delicio','love','delici','friendli','amaz','tasti',\n",
    "            'atmospher','authent','fantast','quick''perfect','qualiti','wonder','cheap',\n",
    "            'fresh','tast','surpris','cozi','reason','cosi','better','worth','famili','fast',\n",
    "            'pleasant','awesom','super','like','real','beauti','relax','recommend','enjoy',\n",
    "            'superb','special','yummi','welcom','cool','fabul','interest','healthi','quiet',\n",
    "            'favourit','delight','high','brilliant','warm','favorit','charm','outstand','cute',\n",
    "            'pricey','pretti','curri','happi','highli','ambianc','vibe','heart','comfort','incred',\n",
    "            'modern','ambienc','fair','conveni','uniqu','surprisingli','clean','lover','heaven','chill',\n",
    "            'sweet','fare','quirki','honest','genuin','class','true','attent','stylish','trendi','return',\n",
    "            'popular','creativ','inexpens','flavor','romant','hospit','satisfi','atmosph','gorgeou','joint',\n",
    "            'eleg','proper','usual','intim','treasur','gourmet','athen','truli','sure','reliabl','ideal',\n",
    "            'ingredi','host','unusu','celebr','care','hearti','effici','perfectli','fanci','pleasur','smile',\n",
    "            'thank','exquisit','fashion','paradis','refresh','pleasantli','jewel','lucki','freshli','fairli',\n",
    "            'spectacular','innov','funki','magic','supper','unbeliev','feast','rustic','smoothi','entertain',\n",
    "            'michelin','highlight','calm','ambient','posit','atmo','correct','inspir','dream','familiar','glad',\n",
    "           'friendliest','greatest','nicest','wholesom','tranquil','comfi','attract','amazingli','frendli',\n",
    "            'pleasent','flavoursom','yumm','excellen','atmosfer','luxuri','royal','freshest','perfecto','tremend',\n",
    "            'deliciu','freindli','frindli','amateur','nicer','greater','brilliantli','coolest','respect','goood']\n",
    "negative = ['disapoint','dissapoint','worst','tasteless','rat','horribl','prici','crepe','problem','wast',\n",
    "            'terribl','bore','mediocr','dissapoint','rude','overpric','disappoint','lack','noisi','slow',\n",
    "            'expens','poor','disgust','avoid','trap','shame','unfriendli','bewar','dirti','unpleas','unpleas',\n",
    "            'underwhelm','rubbish','weird','worthi','scam','poorli','expensi','complaint','cheater','dishonest',\n",
    "            'unwelcom','nope','uninterest','rough','horrend','rudest','disrespect','horrif','pour','slowest',\n",
    "            'jerk','impolit']\n",
    "neutral = ['averag','decent','ordinari','regular','simpl','alright','typic','okay','normal','casual','middl',\n",
    "           'simpli','classi','standard']\n",
    "\n",
    "# создадим признаки с эмоциональной окраской текста отзыва\n",
    "df['positive'] = df['Text_clean'].apply(lambda x: 1 if len(set(x) & set(positive))>0  else 0).astype('float64')\n",
    "df['negative'] = df['Text_clean'].apply(lambda x: 1 if len(set(x) & set(negative))>0  else 0).astype('float64')\n",
    "df['neutral'] = df['Text_clean'].apply(lambda x: 1 if len(set(x) & set(neutral))>0  else 0).astype('float64')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# убираем не нужные для модели признаки\n",
    "df.drop(['ID_TA','URL_TA','Dates'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# добавим dummy признак по  городам\n",
    "df = pd.concat([df,df_dum_Cit], axis=1)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_columns = [s for s in df.columns if df[s].dtypes == 'object']\n",
    "df.drop(object_columns, axis = 1, inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Стандартизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я пока не очень разобралась, как обрабатывать выбросы, поэтому воспользуюсь библиотекой для стандартизации RobustScaler, который не чувствителен к выбросам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для стандартизации\n",
    "def RobustScaler_column(d_col):\n",
    "    scaler = RobustScaler()\n",
    "    scaler.fit(df[[d_col]])\n",
    "    return scaler.transform(df[[d_col]])\n",
    "# стандартизируем все столбцы кроме целевой и Sample\n",
    "for i  in list(df.columns):\n",
    "    if i not in ['Rating','sample']:\n",
    "        df[i] = RobustScaler_column(i)\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.FacetGrid(df[['cuis_isNaN',\n",
    "                     'price_isNaN',\n",
    "                     'Number_of_Reviews_isNAN',\n",
    "                     'no_review',\n",
    "                     'Rating']].melt(\n",
    "                         id_vars=[\"Rating\"],\n",
    "                         var_name=\"feature\",\n",
    "                         value_name=\"value\"), col=\"feature\").map(sns.boxplot, \"value\", \"Rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.FacetGrid(df[['positive',\n",
    "                     'negative',\n",
    "                     'neutral',\n",
    "                     'one_Review',\n",
    "                     'Rating']].melt(\n",
    "                         id_vars=[\"Rating\"],\n",
    "                         var_name=\"feature\",\n",
    "                         value_name=\"value\"), col=\"feature\").map(sns.boxplot, \"value\", \"Rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.FacetGrid(df[['econom',\n",
    "                     'regular',\n",
    "                     'niche_Cuisine_Style',\n",
    "                     'luxury',\n",
    "                     'Rating']].melt(\n",
    "                         id_vars=[\"Rating\"],\n",
    "                         var_name=\"feature\",\n",
    "                         value_name=\"value\"), col=\"feature\").map(sns.boxplot, \"value\", \"Rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим распределение признака"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:08:32.927508Z",
     "iopub.status.busy": "2021-09-28T19:08:32.927232Z",
     "iopub.status.idle": "2021-09-28T19:08:33.43552Z",
     "shell.execute_reply": "2021-09-28T19:08:33.434214Z",
     "shell.execute_reply.started": "2021-09-28T19:08:32.92747Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (10,7)\n",
    "df_train['Ranking'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас много ресторанов, которые не дотягивают и до 2500 места в своем городе, а что там по городам?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:08:33.437976Z",
     "iopub.status.busy": "2021-09-28T19:08:33.437335Z",
     "iopub.status.idle": "2021-09-28T19:08:33.840136Z",
     "shell.execute_reply": "2021-09-28T19:08:33.83937Z",
     "shell.execute_reply.started": "2021-09-28T19:08:33.43765Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train['City'].value_counts(ascending=True).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А кто-то говорил, что французы любят поесть=) Посмотрим, как изменится распределение в большом городе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:08:33.842223Z",
     "iopub.status.busy": "2021-09-28T19:08:33.841666Z",
     "iopub.status.idle": "2021-09-28T19:08:34.283396Z",
     "shell.execute_reply": "2021-09-28T19:08:34.282257Z",
     "shell.execute_reply.started": "2021-09-28T19:08:33.842162Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train['Ranking'][df_train['City'] =='London'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:08:34.285617Z",
     "iopub.status.busy": "2021-09-28T19:08:34.285077Z",
     "iopub.status.idle": "2021-09-28T19:08:36.86833Z",
     "shell.execute_reply": "2021-09-28T19:08:36.867348Z",
     "shell.execute_reply.started": "2021-09-28T19:08:34.285409Z"
    }
   },
   "outputs": [],
   "source": [
    "# посмотрим на топ 10 городов\n",
    "for x in (df_train['City'].value_counts())[0:10].index:\n",
    "    df_train['Ranking'][df_train['City'] == x].hist(bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получается, что Ranking имеет нормальное распределение, просто в больших городах больше ресторанов, из-за мы этого имеем смещение.\n",
    "\n",
    ">Подумайте как из этого можно сделать признак для вашей модели. Я покажу вам пример, как визуализация помогает находить взаимосвязи. А далее действуйте без подсказок =) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим распределение целевой переменной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:08:36.870209Z",
     "iopub.status.busy": "2021-09-28T19:08:36.869922Z",
     "iopub.status.idle": "2021-09-28T19:08:37.092077Z",
     "shell.execute_reply": "2021-09-28T19:08:37.091027Z",
     "shell.execute_reply.started": "2021-09-28T19:08:36.870158Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train['Rating'].value_counts(ascending=True).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим распределение целевой переменной относительно признака"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:08:37.094211Z",
     "iopub.status.busy": "2021-09-28T19:08:37.093844Z",
     "iopub.status.idle": "2021-09-28T19:08:37.540305Z",
     "shell.execute_reply": "2021-09-28T19:08:37.539277Z",
     "shell.execute_reply.started": "2021-09-28T19:08:37.094153Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train['Ranking'][df_train['Rating'] == 5].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:08:37.545051Z",
     "iopub.status.busy": "2021-09-28T19:08:37.544696Z",
     "iopub.status.idle": "2021-09-28T19:08:37.984179Z",
     "shell.execute_reply": "2021-09-28T19:08:37.983217Z",
     "shell.execute_reply.started": "2021-09-28T19:08:37.54499Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train['Ranking'][df_train['Rating'] < 4].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И один из моих любимых - [корреляция признаков](https://ru.wikipedia.org/wiki/Корреляция)\n",
    "На этом графике уже сейчас вы сможете заметить, как признаки связаны между собой и с целевой переменной."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:08:37.986306Z",
     "iopub.status.busy": "2021-09-28T19:08:37.985736Z",
     "iopub.status.idle": "2021-09-28T19:08:40.677165Z",
     "shell.execute_reply": "2021-09-28T19:08:40.676116Z",
     "shell.execute_reply.started": "2021-09-28T19:08:37.986218Z"
    }
   },
   "outputs": [],
   "source": [
    "corrs = df.corr()\n",
    "corrs.columns\n",
    "figure = ff.create_annotated_heatmap(\n",
    "    z=corrs.values,\n",
    "    x=list(corrs.columns),\n",
    "    y=list(corrs.index),\n",
    "    annotation_text=corrs.round(2).values,\n",
    "    showscale=True)\n",
    "figure.update_layout(\n",
    "    autosize=False,\n",
    "    width=2500,\n",
    "    height=1500,)\n",
    "\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вообще благодаря визуализации в этом датасете можно узнать много интересных фактов, например:\n",
    "* где больше Пицерий в Мадриде или Лондоне?\n",
    "* в каком городе кухня ресторанов более разнообразна?\n",
    "\n",
    "придумайте свои вопрос и найдите на него ответ в данных)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Подбор параметров для модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# # блок тестирования оптимального набора\n",
    "list_ofAllColumnsSortImportant = list(feat_importances.nlargest(len(train_data.columns)-1).index)\n",
    "MAE = 0.1724375\n",
    "min_MAE = round(MAE,3)\n",
    "print(f\"min_MAE = {min_MAE}\")\n",
    "remove_list = []\n",
    "log = []\n",
    "delta =0.002\n",
    "\n",
    "for i in range(0,len(list_ofAllColumnsSortImportant),1):\n",
    "    col = list_ofAllColumnsSortImportant[i]\n",
    "    print(f\"{i}.{col}\")\n",
    "    ###\n",
    "\n",
    "    X = train_data.drop(['Rating']+[col], axis=1)\n",
    "    y = train_data['Rating']\n",
    "\n",
    "    # Воспользуемся специальной функцие train_test_split для разбивки тестовых данных\n",
    "    # выделим 20% данных на валидацию (параметр test_size)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "    print(df.shape, y.shape, X.shape, X_train.shape, X_test.shape)\n",
    "    \n",
    "    model_ = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)\n",
    "    model_.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model_.predict(X_test)\n",
    "    for i in range(y_pred.size):\n",
    "        y_pred[i]=rating(y_pred[i])\n",
    "        \n",
    "    temp_MAE = metrics.mean_absolute_error(y_test, y_pred)\n",
    "     ###\n",
    "    print(temp_MAE)\n",
    "    log.append([col, temp_MAE])\n",
    "    if round(temp_MAE,3) <= min_MAE-delta:\n",
    "        remove_list.append(col)\n",
    "        print(f\"удаляем:= {col}\")\n",
    "    else:\n",
    "        print(f\"не удаляем:= {col}\")\n",
    "print(f\"i={i}\")\n",
    "print(f\"remove_list: {remove_list}\")\n",
    "print(f\"log_list: {log}\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Блок подбора параметров задокументирован, чтобы не запускать снова долгие расчеты, результат сохранен в log_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_list = [['norm_Ranking_on_Rest_in_City', 0.172875], ['Number_of_Reviews', 0.2376875], \n",
    "            ['Rome', 0.17325], ['norm_Ranking_on_Pop_in_City', 0.17325], \n",
    "            ['Number_of_Reviews_isNAN', 0.173875], ['Ranking', 0.1729375], \n",
    "            ['time_to_now', 0.1773125], ['Period_rev', 0.1751875], ['code_Cuisine', 0.1716875], \n",
    "            ['code_City', 0.1735625], ['count_franchise', 0.1724375], ['Count_cuisine', 0.1731875], \n",
    "            ['Text_clean_len', 0.173375], ['Madrid', 0.173125], ['Population', 0.171875], \n",
    "            ['mean_Ranking_on_City', 0.173125], ['count_rest_in_city', 0.173125], \n",
    "            ['negative', 0.172375], ['regular_Cuisine_Style', 0.1725625], ['positive', 0.171], \n",
    "            ['regular', 0.1728125], ['Edinburgh', 0.17325], ['price_isNaN', 0.1736875], \n",
    "            ['econom', 0.1734375], ['Dates_num', 0.1733125], ['Mediterranean', 0.1729375], \n",
    "            ['Spanish', 0.17175], ['Italian', 0.1723125], ['European', 0.174], ['one_Review', 0.1721875], \n",
    "            ['Vegetarian Friendly', 0.1728125], ['French', 0.1726875], ['Pizza', 0.172875], \n",
    "            ['Cafe', 0.173625], ['Hamburg', 0.1726875], ['neutral', 0.1733125], ['Stockholm', 0.1733125], \n",
    "            ['Amsterdam', 0.172625], ['Asian', 0.1731875], ['niche_Cuisine_Style', 0.17325], \n",
    "            ['Oporto', 0.1719375], ['Vegan Options', 0.1733125], ['cuis_isNaN', 0.173625], \n",
    "            ['Fast Food', 0.1716875], ['Berlin', 0.1725], ['Pub', 0.17275], ['Bar', 0.1719375], \n",
    "            ['luxury', 0.172], ['Bratislava', 0.1723125], ['Milan', 0.1720625], ['Munich', 0.172375], \n",
    "            ['Prague', 0.172875], ['Dublin', 0.1733125], ['no_review', 0.1729375], \n",
    "            ['Gluten Free Options', 0.173625], ['Krakow', 0.172625], ['Budapest', 0.1749375], \n",
    "            ['Barcelona', 0.171875], ['Athens', 0.1718125], ['Vienna', 0.173], ['Paris', 0.173], \n",
    "            ['Warsaw', 0.1738125], ['Lisbon', 0.1726875], ['Lyon', 0.1725625], ['Copenhagen', 0.17375], \n",
    "            ['Helsinki', 0.173], ['Geneva', 0.1735625], ['Brussels', 0.1725], ['London', 0.1730625], \n",
    "            ['Oslo', 0.172375], ['Zurich', 0.17275], ['Luxembourg', 0.174], ['Ljubljana', 0.1725]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лишних признаков не было выявлено"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "Теперь, для удобства и воспроизводимости кода, завернем всю обработку в одну большую функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_data(df_input):\n",
    "    '''includes several functions to pre-process the predictor data.'''\n",
    "    \n",
    "    df_output = df_input.copy()\n",
    "    df_output.columns = [column.replace(' ','_') for column in df_output.columns]\n",
    "    ##### Обработка признаков\n",
    "    \n",
    "    ###  Restaurant_id\n",
    "    count_franchise = df_output['Restaurant_id'].value_counts()\n",
    "    df_output['count_franchise'] = df_output['Restaurant_id'].apply(lambda x: count_franchise[x])\n",
    "    \n",
    "    ###  City\n",
    "    le = LabelEncoder()\n",
    "    le.fit(df_output['City'])\n",
    "    df_output['code_City'] = le.transform(df_output['City'])\n",
    "\n",
    "    count_rest_in_city = df_output['City'].value_counts()\n",
    "    df_output['count_rest_in_city'] = df_output['City'].apply(lambda x: count_rest_in_city[x])\n",
    "    \n",
    "    city_dict = {'Paris': 2148.327, 'Helsinki':656.611, 'Edinburgh':488.1, 'London':8961.989, 'Bratislava':437.725, \n",
    "             'Lisbon':505.526,'Budapest':1752.286, 'Stockholm':961.609, 'Rome':2870.500, 'Milan':1378.689, \n",
    "             'Munich':1471.508,'Hamburg':1841.179,'Prague':1335.084, 'Vienna':1897.491, 'Dublin':1173.179, \n",
    "             'Barcelona':1664.182, 'Brussels':185.103,'Madrid':3266.126,'Oslo':673.469, 'Amsterdam':872.757, \n",
    "             'Berlin':3644.826, 'Lyon': 506.615, 'Athens':664.046, 'Warsaw':1789.620,\n",
    "             'Oporto':231.962, 'Krakow':769.307, 'Copenhagen':1358.608, 'Luxembourg':636.739, \n",
    "             'Zurich':1407.572, 'Geneva':620.131,'Ljubljana':284.355}\n",
    "\n",
    "    df_output['Population'] = df_output.apply(lambda row: city_dict[row['City']], axis=1)\n",
    "    df_dum_Cit = pd.get_dummies(df_output['City'], dummy_na=False).astype('float64')\n",
    "    df_output = pd.concat([df_output,df_dum_Cit], axis=1)\n",
    "    \n",
    "    ###  Cuisine_Style\n",
    "    le.fit(df_output.Cuisine_Style)\n",
    "    df_output['code_Cuisine'] = le.transform(df_output.Cuisine_Style)\n",
    "    \n",
    "    df_output['Cuisine_Style'] = df_output.Cuisine_Style.fillna(\"['cuis_isNaN']\")\n",
    "    df_output['Cuisine_Style'] = df_output.Cuisine_Style.str.findall(r\"'(\\b.*?\\b)'\")\n",
    "    df_output['Count_cuisine'] = df_output.Cuisine_Style.apply(lambda x: len(x))\n",
    "\n",
    "    def list_unrar(list_of_lists):\n",
    "        result=[]\n",
    "        for lst in list_of_lists:\n",
    "            result.extend(lst)\n",
    "        return result\n",
    "\n",
    "    temp_list = df_output.Cuisine_Style.tolist()\n",
    "    temp_Counter = Counter(list_unrar(temp_list))\n",
    "    # список самых популярных кухонь\n",
    "    list_of_popular = [x[0] for x in temp_Counter.most_common(15)]\n",
    "    # список довольно редких кухонь (менее 90 упоминаний)\n",
    "    list_of_niche = [x[0] for x in temp_Counter.most_common()[-50:]]\n",
    "    # список кухонь средней частоты (все остальные)\n",
    "    list_of_regular = [x[0] for x in temp_Counter.most_common()[15:-50]]\n",
    "    df_output['niche_Cuisine_Style'] = df_output.Cuisine_Style.apply(lambda x: 1 if len(set(x) & set(list_of_niche))>0  else 0).astype('float64')\n",
    "    df_output['regular_Cuisine_Style'] = df_output.Cuisine_Style.apply(lambda x: 1 if len(set(x) & set(list_of_regular))>0  else 0).astype('float64')\n",
    "    # а самые популярные 15 кухонь выведем в отдельные категории\n",
    "    for cuisine in list_of_popular:\n",
    "        df_output[cuisine] = df_output.Cuisine_Style.apply(lambda x: 1 if cuisine in x else 0 ).astype('float64')\n",
    "        \n",
    "    ###  Ranking\n",
    "    mean_Ranking_on_City = df_output.groupby(['City'])['Ranking'].mean()\n",
    "    df_output['mean_Ranking_on_City'] = df_output.City.apply(lambda x: mean_Ranking_on_City[x])\n",
    "    df_output['norm_Ranking_on_Rest_in_City'] = (df_output.Ranking - df_output.mean_Ranking_on_City) / df_output.count_rest_in_city\n",
    "    df_output['norm_Ranking_on_Pop_in_City'] = (df_output.Ranking - df_output.mean_Ranking_on_City) / df_output.Population\n",
    "    \n",
    "    ###  Price_Range\n",
    "    # заполним пропуски и выделим ценовые категории\n",
    "    def price_range(x):\n",
    "        if x=='$':\n",
    "            return 'econom'\n",
    "        elif x=='$$ - $$$':\n",
    "            return 'regular'\n",
    "        elif x=='$$$$':\n",
    "            return 'luxury'\n",
    "        return \"price_isNaN\"\n",
    "\n",
    "    df_output.Price_Range = df_output.Price_Range.apply(price_range)\n",
    "    price_list = list(set(df_output.Price_Range.tolist()))\n",
    "    for price in price_list:\n",
    "        df_output[price] = df_output.Price_Range.apply(lambda x: 1 if price in x else 0 ).astype('float64') \n",
    "        \n",
    "    ###  Number_of_Reviews\n",
    "    df_output['Number_of_Reviews_isNAN'] = pd.isna(df_output['Number_of_Reviews']).astype('float64')\n",
    "    # заполним пропуски медианным по городу значением\n",
    "    grp = df_output.groupby(['City'])\n",
    "    df_output.Number_of_Reviews = grp.Number_of_Reviews.apply(lambda x: x.fillna(x.median()))\n",
    "    \n",
    "    ###  Reviews\n",
    "    df_output['Reviews'] = df_output['Reviews'].fillna(\"[[], []]\")\n",
    "    # Обработаем даты\n",
    "    df_output['Dates'] = df_output.Reviews.str.findall(r'\\d{2}/\\d{2}/\\d{4}')\n",
    "    df_output['Dates_num'] = df_output['Dates'].apply(len)\n",
    "    # сохраним рестораны с одним отзывом\n",
    "    df_output['one_Review'] = (df_output['Dates_num']==1).astype('float64')\n",
    "    # функция определяет насколько давно был сделан самый свежий отзыв\n",
    "    def time_to_now(row):\n",
    "        if row['Dates'] == []:\n",
    "            return None\n",
    "        return datetime.datetime.now() - pd.to_datetime(row['Dates']).max()\n",
    "    # функция для определения перерыва между отзывами\n",
    "    def time_between_Reviews(row):\n",
    "        if row['Dates'] == []:\n",
    "            return None\n",
    "        return pd.to_datetime(row['Dates']).max() - pd.to_datetime(row['Dates']).min()\n",
    "\n",
    "    df_output['time_to_now'] = df_output.apply(time_to_now, axis = 1).dt.days\n",
    "    df_output['Period_rev'] = df_output[df_output['Dates_num']==2].apply(time_between_Reviews, axis = 1).dt.days\n",
    "    grp = df_output.groupby(['City'])\n",
    "    df_output.time_to_now = grp.time_to_now.apply(lambda x: x.fillna(x.median()))\n",
    "    df_output.Period_rev = grp.Period_rev.apply(lambda x: x.fillna(x.median()))\n",
    "    # обработка текста\n",
    "    # функция выдаст список слов отзыва сокращенных до корневой формы\n",
    "    def lemmatize_stemming(text):\n",
    "        return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))  \n",
    "    # функция удалит шумовые слова из текста\n",
    "    def preprocess(text):\n",
    "        result=[]\n",
    "        for token in gensim.utils.simple_preprocess(text) :\n",
    "            if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "                result.append(lemmatize_stemming(token))\n",
    "        return result\n",
    "    df_output['no_review'] = (df_output['Dates_num']==0).astype('float64')\n",
    "    df_output['Text_clean'] = df_output['Reviews'].apply(lemmatize_stemming)\n",
    "    df_output['Text_clean'] = df_output['Reviews'].apply(preprocess)\n",
    "    df_output['Text_clean_len'] = df_output['Text_clean'].apply(len) \n",
    "    \n",
    "    # выделим списки популярных слов в соответствии с оценкой\n",
    "    positive = ['good','great','nice','best','excel','delicio','love','delici','friendli','amaz','tasti',\n",
    "                'atmospher','authent','fantast','quick''perfect','qualiti','wonder','cheap',\n",
    "                'fresh','tast','surpris','cozi','reason','cosi','better','worth','famili','fast',\n",
    "                'pleasant','awesom','super','like','real','beauti','relax','recommend','enjoy',\n",
    "                'superb','special','yummi','welcom','cool','fabul','interest','healthi','quiet',\n",
    "                'favourit','delight','high','brilliant','warm','favorit','charm','outstand','cute',\n",
    "                'pricey','pretti','curri','happi','highli','ambianc','vibe','heart','comfort','incred',\n",
    "                'modern','ambienc','fair','conveni','uniqu','surprisingli','clean','lover','heaven','chill',\n",
    "                'sweet','fare','quirki','honest','genuin','class','true','attent','stylish','trendi','return',\n",
    "                'popular','creativ','inexpens','flavor','romant','hospit','satisfi','atmosph','gorgeou','joint',\n",
    "                'eleg','proper','usual','intim','treasur','gourmet','athen','truli','sure','reliabl','ideal',\n",
    "                'ingredi','host','unusu','celebr','care','hearti','effici','perfectli','fanci','pleasur','smile',\n",
    "                'thank','exquisit','fashion','paradis','refresh','pleasantli','jewel','lucki','freshli','fairli',\n",
    "                'spectacular','innov','funki','magic','supper','unbeliev','feast','rustic','smoothi','entertain',\n",
    "                'michelin','highlight','calm','ambient','posit','atmo','correct','inspir','dream','familiar','glad',\n",
    "               'friendliest','greatest','nicest','wholesom','tranquil','comfi','attract','amazingli','frendli',\n",
    "                'pleasent','flavoursom','yumm','excellen','atmosfer','luxuri','royal','freshest','perfecto','tremend',\n",
    "                'deliciu','freindli','frindli','amateur','nicer','greater','brilliantli','coolest','respect','goood']\n",
    "    negative = ['disapoint','dissapoint','worst','tasteless','rat','horribl','prici','crepe','problem','wast',\n",
    "                'terribl','bore','mediocr','dissapoint','rude','overpric','disappoint','lack','noisi','slow',\n",
    "                'expens','poor','disgust','avoid','trap','shame','unfriendli','bewar','dirti','unpleas','unpleas',\n",
    "                'underwhelm','rubbish','weird','worthi','scam','poorli','expensi','complaint','cheater','dishonest',\n",
    "                'unwelcom','nope','uninterest','rough','horrend','rudest','disrespect','horrif','pour','slowest',\n",
    "                'jerk','impolit']\n",
    "    neutral = ['averag','decent','ordinari','regular','simpl','alright','typic','okay','normal','casual','middl',\n",
    "               'simpli','classi','standard']\n",
    "\n",
    "    # создадим признаки с эмоциональной окраской текста отзыва\n",
    "    df_output['positive'] = df_output['Text_clean'].apply(lambda x: 1 if len(set(x) & set(positive))>0  else 0).astype('float64')\n",
    "    df_output['negative'] = df_output['Text_clean'].apply(lambda x: 1 if len(set(x) & set(negative))>0  else 0).astype('float64')\n",
    "    df_output['neutral'] = df_output['Text_clean'].apply(lambda x: 1 if len(set(x) & set(neutral))>0  else 0).astype('float64')\n",
    "\n",
    "    # убираем не нужные для модели признаки\n",
    "    df_output.drop(['ID_TA','URL_TA','Dates', 'Restaurant_id'], axis = 1, inplace=True)\n",
    "    # удалим все нечисловые колонки\n",
    "    object_columns = [s for s in df_output.columns if df_output[s].dtypes == 'object']\n",
    "    df_output.drop(object_columns, axis = 1, inplace=True)\n",
    "    \n",
    "    ###  Стандартизация\n",
    "    # функция для стандартизации\n",
    "    def RobustScaler_column(d_col):\n",
    "        scaler = RobustScaler()\n",
    "        scaler.fit(df_output[[d_col]])\n",
    "        return scaler.transform(df_output[[d_col]])\n",
    "    # стандартизируем все столбцы кроме целевой и Sample\n",
    "    for i  in list(df_output.columns):\n",
    "        if i not in ['Rating','sample']:\n",
    "            df_output[i] = RobustScaler_column(i)\n",
    "            \n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Запускаем и проверяем что получилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:08:43.904058Z",
     "iopub.status.busy": "2021-09-28T19:08:43.903604Z",
     "iopub.status.idle": "2021-09-28T19:09:38.057279Z",
     "shell.execute_reply": "2021-09-28T19:09:38.056375Z",
     "shell.execute_reply.started": "2021-09-28T19:08:43.90398Z"
    }
   },
   "outputs": [],
   "source": [
    "df_preproc = preproc_data(data)\n",
    "df_preproc.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:09:38.058845Z",
     "iopub.status.busy": "2021-09-28T19:09:38.05857Z",
     "iopub.status.idle": "2021-09-28T19:09:38.085954Z",
     "shell.execute_reply": "2021-09-28T19:09:38.084869Z",
     "shell.execute_reply.started": "2021-09-28T19:09:38.058797Z"
    }
   },
   "outputs": [],
   "source": [
    "df_preproc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:09:38.091997Z",
     "iopub.status.busy": "2021-09-28T19:09:38.09169Z",
     "iopub.status.idle": "2021-09-28T19:09:38.128947Z",
     "shell.execute_reply": "2021-09-28T19:09:38.127875Z",
     "shell.execute_reply.started": "2021-09-28T19:09:38.091948Z"
    }
   },
   "outputs": [],
   "source": [
    "# Теперь выделим тестовую часть\n",
    "train_data = df_preproc.query('sample == 1').drop(['sample'], axis=1)\n",
    "test_data = df_preproc.query('sample == 0').drop(['sample'], axis=1)\n",
    "\n",
    "y = train_data.Rating.values            # наш таргет\n",
    "X = train_data.drop(['Rating'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Перед тем как отправлять наши данные на обучение, разделим данные на еще один тест и трейн, для валидации. \n",
    "Это поможет нам проверить, как хорошо наша модель работает, до отправки submissiona на kaggle.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:09:38.131009Z",
     "iopub.status.busy": "2021-09-28T19:09:38.130704Z",
     "iopub.status.idle": "2021-09-28T19:09:38.150796Z",
     "shell.execute_reply": "2021-09-28T19:09:38.149601Z",
     "shell.execute_reply.started": "2021-09-28T19:09:38.130955Z"
    }
   },
   "outputs": [],
   "source": [
    "# Воспользуемся специальной функцие train_test_split для разбивки тестовых данных\n",
    "# выделим 20% данных на валидацию (параметр test_size)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:09:38.152751Z",
     "iopub.status.busy": "2021-09-28T19:09:38.152474Z",
     "iopub.status.idle": "2021-09-28T19:09:38.15997Z",
     "shell.execute_reply": "2021-09-28T19:09:38.158933Z",
     "shell.execute_reply.started": "2021-09-28T19:09:38.152706Z"
    }
   },
   "outputs": [],
   "source": [
    "# проверяем\n",
    "test_data.shape, train_data.shape, X.shape, X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model \n",
    "Сам ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:09:38.161617Z",
     "iopub.status.busy": "2021-09-28T19:09:38.161351Z",
     "iopub.status.idle": "2021-09-28T19:09:38.276616Z",
     "shell.execute_reply": "2021-09-28T19:09:38.275518Z",
     "shell.execute_reply.started": "2021-09-28T19:09:38.161574Z"
    }
   },
   "outputs": [],
   "source": [
    "# Импортируем необходимые библиотеки:\n",
    "from sklearn.ensemble import RandomForestRegressor # инструмент для создания и обучения модели\n",
    "from sklearn import metrics # инструменты для оценки точности модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:09:38.278346Z",
     "iopub.status.busy": "2021-09-28T19:09:38.278054Z",
     "iopub.status.idle": "2021-09-28T19:09:38.283258Z",
     "shell.execute_reply": "2021-09-28T19:09:38.282358Z",
     "shell.execute_reply.started": "2021-09-28T19:09:38.278272Z"
    }
   },
   "outputs": [],
   "source": [
    "# Создаём модель (НАСТРОЙКИ НЕ ТРОГАЕМ)\n",
    "model = RandomForestRegressor(n_estimators=100, verbose=1, n_jobs=-1, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:09:38.284725Z",
     "iopub.status.busy": "2021-09-28T19:09:38.284487Z",
     "iopub.status.idle": "2021-09-28T19:09:47.592566Z",
     "shell.execute_reply": "2021-09-28T19:09:47.591658Z",
     "shell.execute_reply.started": "2021-09-28T19:09:38.284691Z"
    }
   },
   "outputs": [],
   "source": [
    "# Обучаем модель на тестовом наборе данных\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Используем обученную модель для предсказания рейтинга ресторанов в тестовой выборке.\n",
    "# Предсказанные значения записываем в переменную y_pred\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "def rating(prediction):\n",
    "        if prediction < 0.25:\n",
    "            return 0\n",
    "        elif 0.25 < prediction <= 0.75:\n",
    "            return 0.5\n",
    "        elif 0.75 < prediction <= 1.25:\n",
    "            return 1\n",
    "        elif 1.25 <prediction <= 1.75:\n",
    "            return 1.5\n",
    "        elif 1.75 < prediction <= 2.25:\n",
    "            return 2\n",
    "        elif 2.25 < prediction <= 2.75:\n",
    "            return 2.5\n",
    "        elif 2.75 < prediction <= 3.25:\n",
    "            return 3\n",
    "        elif 3.25 < prediction <= 3.75:\n",
    "            return 3.5\n",
    "        elif 3.75 < prediction <= 4.25:\n",
    "            return 4\n",
    "        elif 4.25 < prediction <= 4.75:\n",
    "            return 4.5\n",
    "        else:\n",
    "            return 5\n",
    "        \n",
    "for i in range(y_pred.size):\n",
    "        y_pred[i]=rating(y_pred[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:09:47.594374Z",
     "iopub.status.busy": "2021-09-28T19:09:47.594068Z",
     "iopub.status.idle": "2021-09-28T19:09:47.599966Z",
     "shell.execute_reply": "2021-09-28T19:09:47.599168Z",
     "shell.execute_reply.started": "2021-09-28T19:09:47.594321Z"
    }
   },
   "outputs": [],
   "source": [
    "# Сравниваем предсказанные значения (y_pred) с реальными (y_test), и смотрим насколько они в среднем отличаются\n",
    "# Метрика называется Mean Absolute Error (MAE) и показывает среднее отклонение предсказанных значений от фактических.\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:09:47.601196Z",
     "iopub.status.busy": "2021-09-28T19:09:47.600981Z",
     "iopub.status.idle": "2021-09-28T19:09:48.080953Z",
     "shell.execute_reply": "2021-09-28T19:09:48.08001Z",
     "shell.execute_reply.started": "2021-09-28T19:09:47.601163Z"
    }
   },
   "outputs": [],
   "source": [
    "# в RandomForestRegressor есть возможность вывести самые важные признаки для модели\n",
    "plt.rcParams['figure.figsize'] = (10,10)\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(25).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "Если все устраевает - готовим Submission на кагл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:09:48.083165Z",
     "iopub.status.busy": "2021-09-28T19:09:48.082555Z",
     "iopub.status.idle": "2021-09-28T19:09:48.134305Z",
     "shell.execute_reply": "2021-09-28T19:09:48.133316Z",
     "shell.execute_reply.started": "2021-09-28T19:09:48.083103Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:09:48.136717Z",
     "iopub.status.busy": "2021-09-28T19:09:48.136041Z",
     "iopub.status.idle": "2021-09-28T19:09:48.144768Z",
     "shell.execute_reply": "2021-09-28T19:09:48.143604Z",
     "shell.execute_reply.started": "2021-09-28T19:09:48.136642Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = test_data.drop(['Rating'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:09:48.146435Z",
     "iopub.status.busy": "2021-09-28T19:09:48.146161Z",
     "iopub.status.idle": "2021-09-28T19:09:48.167705Z",
     "shell.execute_reply": "2021-09-28T19:09:48.166654Z",
     "shell.execute_reply.started": "2021-09-28T19:09:48.146391Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:09:48.170606Z",
     "iopub.status.busy": "2021-09-28T19:09:48.169815Z",
     "iopub.status.idle": "2021-09-28T19:09:48.287847Z",
     "shell.execute_reply": "2021-09-28T19:09:48.286933Z",
     "shell.execute_reply.started": "2021-09-28T19:09:48.170368Z"
    }
   },
   "outputs": [],
   "source": [
    "predict_submission = model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:09:48.289866Z",
     "iopub.status.busy": "2021-09-28T19:09:48.289509Z",
     "iopub.status.idle": "2021-09-28T19:09:48.296952Z",
     "shell.execute_reply": "2021-09-28T19:09:48.295963Z",
     "shell.execute_reply.started": "2021-09-28T19:09:48.289807Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(predict_submission.size):\n",
    "        predict_submission[i]=rating(predict_submission[i])\n",
    "predict_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-28T19:09:48.299201Z",
     "iopub.status.busy": "2021-09-28T19:09:48.298578Z",
     "iopub.status.idle": "2021-09-28T19:09:48.508023Z",
     "shell.execute_reply": "2021-09-28T19:09:48.506934Z",
     "shell.execute_reply.started": "2021-09-28T19:09:48.299136Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission['Rating'] = predict_submission\n",
    "sample_submission.to_csv('submission.csv', index=False)\n",
    "sample_submission.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's next?\n",
    "Или что делать, чтоб улучшить результат:\n",
    "* Обработать оставшиеся признаки в понятный для машины формат\n",
    "* Посмотреть, что еще можно извлечь из признаков\n",
    "* Сгенерировать новые признаки\n",
    "* Подгрузить дополнительные данные, например: по населению или благосостоянию городов\n",
    "* Подобрать состав признаков\n",
    "\n",
    "В общем, процесс творческий и весьма увлекательный! Удачи в соревновании!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
